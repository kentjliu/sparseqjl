{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51HftXqlaBMT"
      },
      "source": [
        "# SparseQJL Demo\n",
        "This is a notebook which you can follow to run the same experiments we did. For experiments with Llama models, we recommend using an `A100` runtime for sufficient RAM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JLFrIgiGP-AB"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -q datasets\n",
        "!pip install -q transformers\n",
        "!pip install triton\n",
        "!pip install flash_attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1xxUyt8-QFVj"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!git clone https://github.com/kentjliu/sparseqjl.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnTMn-Q1QHuA",
        "outputId": "25da90b7-c60a-4b2a-e32f-3ffc99c38b32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/sparseqjl\n"
          ]
        }
      ],
      "source": [
        "%cd sparseqjl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5BvUkOGb2liQ"
      },
      "outputs": [],
      "source": [
        "# Set up your Huggingface Token here to load models, For llama models, you need to request to access from Meta first.\n",
        "import os\n",
        "os.environ[\"HF_TOKEN\"] = \"YOUR_HUGGINGFACE_TOKEN\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sw8JgyIu3Hks"
      },
      "source": [
        "## Build kernels (~5 min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Q8tYqp1PkVDq"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!python qjl_kernel/setup.py build_ext --inplace --build-temp=./qjl_kernel/build --build-lib=./qjl_kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9Tyv1BeZjQZ"
      },
      "source": [
        "Ensure kernels are built in the correct directory. The following files should show:\n",
        "* `cuda_qjl_gqa_score.cpython-310-x86_64-linux-gnu.so*`\n",
        "* `cuda_qjl_quant.cpython-310-x86_64-linux-gnu.so*  `\n",
        "* `cuda_qjl_score.cpython-310-x86_64-linux-gnu.so*`\n",
        "* `quantization.cpython-310-x86_64-linux-gnu.so*`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZe1MFGvkR2Q",
        "outputId": "08ad1d22-5872-479b-d6c5-7a8184ff063b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mbuild\u001b[0m/                                               matmul.py\n",
            "\u001b[01;34mcsrc\u001b[0m/                                                new_pack.py\n",
            "\u001b[01;32mcuda_qjl_gqa_score.cpython-310-x86_64-linux-gnu.so\u001b[0m*  qjl_kernel.py\n",
            "\u001b[01;32mcuda_qjl_quant.cpython-310-x86_64-linux-gnu.so\u001b[0m*      \u001b[01;32mquantization.cpython-310-x86_64-linux-gnu.so\u001b[0m*\n",
            "\u001b[01;32mcuda_qjl_score.cpython-310-x86_64-linux-gnu.so\u001b[0m*      setup.py\n"
          ]
        }
      ],
      "source": [
        "%ls qjl_kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyTwXstvBOp6"
      },
      "source": [
        "## Test SparseQJL on Llama\n",
        "Params:\n",
        "\n",
        "* `model_name`: String denoting HuggingFace Llama model path to test. Default: `meta-llama/Llama-2-7b-hf`\n",
        "* `qjl`: Boolean flag denoting whether or not to apply QJL.\n",
        "* `sparsity`: Float between 0 and 1 denoting \\% uniform sparsity with SparseGPT. Default: `0.0`\n",
        "* `wbits`: Int denoting the bit-width for weight quantization. We suggest using a value of `4`. Default: `16` (No quant)\n",
        "* `dtype`: String denoting standard datatype of model. Options are `float16` and `float32`. Default: `float16`.\n",
        "\n",
        "Note:\n",
        "* `meta-llama/Llama-2-7b-hf`: takes about 20 minutes to run with pruning\n",
        "* `meta-llama/Llama-2-13b-hf`: takes about 35 minutes to run with pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZU85hko9QO4u",
        "outputId": "1df1b112-a1e0-4498-e13f-14dbfe37592d"
      },
      "outputs": [],
      "source": [
        "!python llama_sparseqjl.py --model_name \"meta-llama/Llama-2-7b-hf\" \\\n",
        "    --qjl \\\n",
        "    --sparsity 0.5 \\\n",
        "    --wbits 4 \\\n",
        "    --dtype \"float16\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkNo7WPwdOw-"
      },
      "source": [
        "## Test SparseQJL on OPT\n",
        "Params:\n",
        "\n",
        "* `model_name`: String denoting HuggingFace OPT model path to test. Default: `facebook/opt-125m`\n",
        "* `qjl`: Boolean flag denoting whether or not to apply QJL.\n",
        "* `sparsity`: Float between 0 and 1 denoting \\% uniform sparsity with SparseGPT. Default: `0.0`\n",
        "* `wbits`: Int denoting the bit-width for weight quantization. We suggest using a value of `4`. Default: `16` (No quant)\n",
        "* `dtype`: String denoting standard datatype of model. Options are `float16` and `float32`. Default: `float16`.\n",
        "\n",
        "Note:\n",
        "\n",
        "* Currently, our implementation of SparseQJL on OPT is still not 100\\% refined, hence the extremely high perplexity scores. We will continue to resolve the issue and make updates to this repo.\n",
        "* When prompted to run custom code, answer `y` in the CLI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aiEP6KB2JIQ",
        "outputId": "0dd7853a-fb6c-4db5-b0f5-bd06c3002846"
      },
      "outputs": [],
      "source": [
        "!python opt_sparseqjl.py --model_name \"facebook/opt-350m\" \\\n",
        "    --qjl \\\n",
        "    --sparsity 0.5 \\\n",
        "    --wbits 4 \\\n",
        "    --dtype \"float16\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
